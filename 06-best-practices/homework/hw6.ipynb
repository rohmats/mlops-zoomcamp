{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Refactoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the \"main\" block from which we'll invoke the main function. How does the if statement that we use for this looks like?\n",
    "\n",
    "`if __name__ == \"__main__\":`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code `batch.py ` for Q1\n",
    "<details>\n",
    "<summary>Click to show/hide code</summary>\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import warnings\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import click\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='sklearn')\n",
    "\n",
    "\n",
    "def generate_output_file_path(year, month):\n",
    " return f'./output/yellow_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "\n",
    "\n",
    "def read_data(filename, categorical):\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df['duration'] = df.duration.dt.total_seconds() / 60\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "\n",
    "    return df, categorical\n",
    "\n",
    "@click.command()\n",
    "@click.option(\n",
    "    '--year',\n",
    "    type=int,\n",
    "    required=True,\n",
    "    help='Year of the trip data'\n",
    ")\n",
    "@click.option(\n",
    "    '--month',\n",
    "    type=int,\n",
    "    required=True,\n",
    "    help='Month of the trip data'\n",
    ")\n",
    "def main(year, month):\n",
    "    with open('./model.bin', 'rb') as f_in:\n",
    "        dv, lr = pickle.load(f_in)\n",
    "\n",
    "    input_file = f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "    df, categorical = read_data(input_file, categorical = ['PULocationID', 'DOLocationID'])\n",
    "\n",
    "    df['ride_id'] = f'{year:04d}/{month:02d}_' + df.index.astype('str')\n",
    "\n",
    "    dicts = df[categorical].to_dict(orient='records')\n",
    "    X_val = dv.transform(dicts)\n",
    "    y_pred = lr.predict(X_val)\n",
    "\n",
    "    print('predicted mean duration:', y_pred.mean())\n",
    "\n",
    "\n",
    "    df_result = pd.DataFrame()\n",
    "    df_result['ride_id'] = df['ride_id']\n",
    "    df_result['predicted_duration'] = y_pred\n",
    "\n",
    "    output_file = generate_output_file_path(year, month)\n",
    "    df_result.to_parquet(output_file, engine='pyarrow', index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted mean duration: 14.292282936862449\n"
     ]
    }
   ],
   "source": [
    "! python batch.py --year 2023 --month 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  ..  yellow_tripdata_2023-04.parquet\n"
     ]
    }
   ],
   "source": [
    "! ls ./output -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Installing pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a folder tests and create two files. One will be the file with tests. We can name it test_batch.py.\n",
    "\n",
    "What should be the other file?\n",
    "\n",
    " `__init__.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install `pytest` to pipenv -dev\n",
    "```bash\n",
    "pipenv install --dev pytest\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! touch tests/__init__.py tests/test_batch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  ..  __init__.py  test_batch.py\n"
     ]
    }
   ],
   "source": [
    "! ls ./tests -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Writing first unit test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many rows should be there in the expected dataframe?\n",
    "\n",
    "* 1\n",
    "* 2 ✅\n",
    "* 3\n",
    "* 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code `batch.py` for Q3:\n",
    "<details>\n",
    "<summary>Click to show/hide code</summary>\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import warnings\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import click\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='sklearn')\n",
    "\n",
    "\n",
    "def generate_output_file_path(year, month):\n",
    " return f'./output/yellow_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "\n",
    "\n",
    "def prepare_data(df, categorical):\n",
    "    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df['duration'] = df.duration.dt.total_seconds() / 60\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "    return df, categorical\n",
    "\n",
    "\n",
    "def read_data(filename, categorical):\n",
    "    df = pd.read_parquet(filename)\n",
    "    return prepare_data(df, categorical)\n",
    "\n",
    "\n",
    "@click.command()\n",
    "@click.option(\n",
    "    '--year',\n",
    "    type=int,\n",
    "    required=True,\n",
    "    help='Year of the trip data'\n",
    ")\n",
    "@click.option(\n",
    "    '--month',\n",
    "    type=int,\n",
    "    required=True,\n",
    "    help='Month of the trip data'\n",
    ")\n",
    "def main(year, month):\n",
    "    with open('./model.bin', 'rb') as f_in:\n",
    "        dv, lr = pickle.load(f_in)\n",
    "\n",
    "    input_file = f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "    df, categorical = read_data(input_file, categorical = ['PULocationID', 'DOLocationID'])\n",
    "\n",
    "    df['ride_id'] = f'{year:04d}/{month:02d}_' + df.index.astype('str')\n",
    "\n",
    "    dicts = df[categorical].to_dict(orient='records')\n",
    "    X_val = dv.transform(dicts)\n",
    "    y_pred = lr.predict(X_val)\n",
    "\n",
    "    print('predicted mean duration:', y_pred.mean())\n",
    "\n",
    "    df_result = pd.DataFrame()\n",
    "    df_result['ride_id'] = df['ride_id']\n",
    "    df_result['predicted_duration'] = y_pred\n",
    "\n",
    "    output_file = generate_output_file_path(year, month)\n",
    "    df_result.to_parquet(output_file, engine='pyarrow', index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted mean duration: 14.292282936862449\n"
     ]
    }
   ],
   "source": [
    "! python batch.py --year 2023 --month 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for `tests/test_batch.py`\n",
    "\n",
    "<details>\n",
    "<summary>Click to show/hide code</summary>\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "from batch import prepare_data\n",
    "\n",
    "\n",
    "def dt(hour, minute, second=0):\n",
    "    return datetime(2023, 1, 1, hour, minute, second)\n",
    "\n",
    "\n",
    "def prepare_test_data():\n",
    "    data = [\n",
    "    (None, None, dt(1, 1), dt(1, 10)),\n",
    "    (1, 1, dt(1, 2), dt(1, 10)),\n",
    "    (1, None, dt(1, 2, 0), dt(1, 2, 59)),\n",
    "    (3, 4, dt(1, 2, 0), dt(2, 2, 1))\n",
    "    ]\n",
    "\n",
    "    columns_test_df = ['PULocationID', 'DOLocationID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    "    test_df = pd.DataFrame(data, columns=columns_test_df)\n",
    "\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "\n",
    "    prepared_test_df, categorical = prepare_data(test_df, categorical)\n",
    "    print(prepared_test_df)\n",
    "\n",
    "    expected_prepared_test_df = [\n",
    "        ('-1', '-1', 9.),\n",
    "        ('1', '1', 8.),\n",
    "    ]\n",
    "    columns_expected_df = ['PULocationID', 'DOLocationID', 'duration']\n",
    "    expected_prepared_test_df = pd.DataFrame(expected_prepared_test_df, columns=columns_expected_df)\n",
    "\n",
    "    catigorial_cols = ['PULocationID', 'DOLocationID']\n",
    "    for col in catigorial_cols:\n",
    "        assert (prepared_test_df[col] == expected_prepared_test_df[col]).all()\n",
    "\n",
    "    float_cols = ['duration']\n",
    "    epsilon = 1e-9\n",
    "    for col in float_cols:\n",
    "        np.allclose(prepared_test_df[col], expected_prepared_test_df[col], atol=epsilon)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_batch import prepare_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PULocationID DOLocationID tpep_pickup_datetime tpep_dropoff_datetime  \\\n",
      "0           -1           -1  2023-01-01 01:01:00   2023-01-01 01:10:00   \n",
      "1            1            1  2023-01-01 01:02:00   2023-01-01 01:10:00   \n",
      "\n",
      "   duration  \n",
      "0       9.0  \n",
      "1       8.0  \n"
     ]
    }
   ],
   "source": [
    "prepare_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. Mocking S3 with Localstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* --backend-store-uri\n",
    "* --profile\n",
    "* --endpoint-url ✅\n",
    "* --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "! touch docker-compose.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code `docker-compose.yaml`\n",
    "<details>\n",
    "<summary>Click to show/hide code</summary>\n",
    "\n",
    "```YAML\n",
    "services:\n",
    "  localstack:\n",
    "    image: localstack/localstack\n",
    "    container_name: localstack\n",
    "    ports:\n",
    "      - \"4566:4566\"\n",
    "    environment:\n",
    "      - SERVICES=s3\n",
    "      - DEBUG=1\n",
    "      - AWS_ACCESS_KEY_ID=dummyAccessKeyId\n",
    "      - AWS_SECRET_ACCESS_KEY=dummySecretAccessKey\n",
    "      - DEFAULT_REGION=us-east-1\n",
    "    volumes:\n",
    "      - \"./localstack:/var/lib/localstack\"\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build localstack docker image\n",
    "\n",
    "`docker compose up -d --build`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dummy AWS credentials\n",
    "```batch\n",
    "mkdir -p ~/.aws\n",
    "nano ~/.aws/credentials\n",
    "```\n",
    "\n",
    "Code for `~/.aws/credentials`\n",
    "<details>\n",
    "<summary>Click to show/hide code</summary>\n",
    "\n",
    "```\n",
    "[default]\n",
    "aws_access_key_id = foo\n",
    "aws_secret_access_key = bar\n",
    "```\n",
    "</details>\n",
    "\n",
    "Set AWS Configuration\n",
    "```batch\n",
    "nano ~/.aws/config\n",
    "```\n",
    "\n",
    "Code for `~/.aws/config`\n",
    "<details>\n",
    "<summary>Click to show/hide code</summary>\n",
    "\n",
    "```\n",
    "[default]\n",
    "region = us-east-1\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install AWS CLI\n",
    "\n",
    "`pipenv install awscli`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-cli/1.33.22 Python/3.10.13 Linux/6.5.0-1022-azure botocore/1.34.131\n"
     ]
    }
   ],
   "source": [
    "! aws --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_bucket: nyc-duration\n"
     ]
    }
   ],
   "source": [
    "! aws --endpoint-url=http://localhost:4566 s3 mb s3://nyc-duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "year, month =  2023, 4\n",
    "\n",
    "input_file = f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "df = pd.read_parquet(input_file)\n",
    "\n",
    "\n",
    "s3_endpoint_url=\"http://localhost:4566\"\n",
    "input_file=f\"s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\"\n",
    "\n",
    "storage_options = {'client_kwargs': {'endpoint_url': s3_endpoint_url}}\n",
    "df.to_parquet(input_file, engine='pyarrow', index=False, storage_options=storage_options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code `batch.py` for Q4\n",
    "<details>\n",
    "<summary>Click to show/hide code</summary>\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import warnings\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import click\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='sklearn')\n",
    "\n",
    "\n",
    "def get_input_path(year, month):\n",
    "    default_input_pattern = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "    input_pattern = os.getenv('INPUT_FILE_PATTERN', default_input_pattern)\n",
    "    return input_pattern.format(year=year, month=month)\n",
    "\n",
    "def get_output_path(year, month):\n",
    "    default_output_pattern = './output/yellow_tripdata_{year:04d}-{month:02d}.parquet'\n",
    "    output_pattern = os.getenv('OUTPUT_FILE_PATTERN', default_output_pattern)\n",
    "    return output_pattern.format(year=year, month=month)\n",
    "\n",
    "def prepare_data(df, categorical):\n",
    "    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df['duration'] = df.duration.dt.total_seconds() / 60\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "    return df, categorical\n",
    "\n",
    "def read_data(year, month, categorical):\n",
    "    options = {}\n",
    "    s3_endpoint_url = os.getenv('S3_ENDPOINT_URL')\n",
    "    input_pattern = os.getenv('INPUT_FILE_PATTERN')\n",
    "    input_file = get_input_path(year, month)\n",
    "\n",
    "    if s3_endpoint_url and input_pattern:\n",
    "        options['storage_options'] = {'client_kwargs': {'endpoint_url': s3_endpoint_url}}\n",
    "        df = pd.read_parquet(input_file, storage_options=options['storage_options'])\n",
    "        print(f'Data loaded from S3, INPUT_FILE_PATTERN is {input_pattern}')\n",
    "    else:\n",
    "        print('else')\n",
    "        df = pd.read_parquet(input_file)\n",
    "        print(f'Data loaded from the internet, INPUT_FILE_PATTERN is {input_pattern}')\n",
    "    return prepare_data(df, categorical)\n",
    "\n",
    "def save_parquet_to_s3(output_file, df):\n",
    "    s3_endpoint_url = os.getenv('S3_ENDPOINT_URL')\n",
    "    output_pattern = os.getenv('OUTPUT_FILE_PATTERN')\n",
    "    options = {}\n",
    "    if s3_endpoint_url and output_pattern:\n",
    "        options['storage_options'] = {'client_kwargs': {'endpoint_url': s3_endpoint_url}}\n",
    "        df.to_parquet(output_file, engine='pyarrow', index=False, storage_options=options['storage_options'])\n",
    "        print(f'File saved to S3, OUTPUT_FILE_PATTERN is {output_pattern}')\n",
    "    else:\n",
    "        df.to_parquet(output_file, engine='pyarrow', index=False)\n",
    "        print(f'File saved locally, OUTPUT_FILE_PATTERN is {output_pattern}')\n",
    "\n",
    "\n",
    "@click.command()\n",
    "@click.option(\n",
    "    '--year',\n",
    "    type=int,\n",
    "    required=True,\n",
    "    help='Year of the trip data'\n",
    ")\n",
    "@click.option(\n",
    "    '--month',\n",
    "    type=int,\n",
    "    required=True,\n",
    "    help='Month of the trip data'\n",
    ")\n",
    "def main(year, month):\n",
    "    with open('./model.bin', 'rb') as f_in:\n",
    "        dv, lr = pickle.load(f_in)\n",
    "\n",
    "    df, categorical = read_data(year, month, categorical = ['PULocationID', 'DOLocationID'])\n",
    "\n",
    "    df['ride_id'] = f'{year:04d}/{month:02d}_' + df.index.astype('str')\n",
    "\n",
    "    dicts = df[categorical].to_dict(orient='records')\n",
    "    X_val = dv.transform(dicts)\n",
    "    y_pred = lr.predict(X_val)\n",
    "\n",
    "    print('predicted mean duration:', y_pred.mean())\n",
    "\n",
    "    df_result = pd.DataFrame()\n",
    "    df_result['ride_id'] = df['ride_id']\n",
    "    df_result['predicted_duration'] = y_pred\n",
    "\n",
    "    output_file = get_output_path(year, month)\n",
    "    save_parquet_to_s3(output_file, df_result)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "! touch run_batch_py_local.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code `run_batch_py_local.sh`\n",
    "<details>\n",
    "<summary>Click to show/hide code</summary>\n",
    "\n",
    "```shell\n",
    "#!/bin/bash\n",
    "\n",
    "# Set environment variables\n",
    "export INPUT_FILE_PATTERN=\"s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\"\n",
    "export OUTPUT_FILE_PATTERN=\"s3://nyc-duration/out/{year:04d}-{month:02d}.parquet\"\n",
    "export S3_ENDPOINT_URL=\"http://localhost:4566\"\n",
    "\n",
    "# Run the Python script with parameters\n",
    "python batch.py --year 2023 --month 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x run_batch_py_local.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from S3, INPUT_FILE_PATTERN is s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\n",
      "predicted mean duration: 14.292282936862449\n",
      "File saved to S3, OUTPUT_FILE_PATTERN is s3://nyc-duration/out/{year:04d}-{month:02d}.parquet\n"
     ]
    }
   ],
   "source": [
    "! ./run_batch_py_local.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view https://app.localstack.cloud/inst/default/resources/s3/nyc-duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running with docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                          docker:default\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (1/1)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 665B                                       0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 665B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.10.9-slim      0.2s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 665B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.10.9-slim      0.3s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 665B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.10.9-slim      0.5s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.8s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 665B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.10.9-slim      0.6s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 665B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.10.9-slim      0.8s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.1s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 665B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.10.9-slim      0.9s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.2s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 665B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.10.9-slim      1.1s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.4s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 665B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.10.9-slim      1.2s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.5s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 665B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.10.9-slim      1.4s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.7s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 665B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.10.9-slim      1.5s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.8s (3/3)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 665B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.10.9-slim      1.7s\n",
      "\u001b[0m\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.9s (6/15)                                         docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 665B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.10.9-slim      1.7s\n",
      "\u001b[0m\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/10] FROM docker.io/library/python:3.10.9-slim@sha256:76dd18d90a3d  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 120B                                          0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.1s (16/16)                                        docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 665B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.10.9-slim      1.7s\n",
      "\u001b[0m\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/10] FROM docker.io/library/python:3.10.9-slim@sha256:76dd18d90a3d  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 120B                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/10] RUN pip install -U pip & pip install pipenv             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/10] COPY [ Pipfile, Pipfile.lock, ./ ]                      0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/10] RUN pipenv install --system --deploy                    0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/10] RUN pip install click                                   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/10] RUN mkdir -p /root/.aws                                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/10] RUN echo \"[default]\\naws_access_key_id = foo\\naws_secr  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 8/10] RUN echo \"[default]\\nregion = us-east-1\" > /root/.aws/  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 9/10] COPY [ batch.py, batch.py ]                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [10/10] COPY [ model.bin, model.bin ]                           0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:cee9a435043179367bc1b888d7e365faf24aa2cca4dda  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/hw6_image                               0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.3s (16/16) FINISHED                               docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 665B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.10.9-slim      1.7s\n",
      "\u001b[0m\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/10] FROM docker.io/library/python:3.10.9-slim@sha256:76dd18d90a3d  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 120B                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/10] RUN pip install -U pip & pip install pipenv             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/10] COPY [ Pipfile, Pipfile.lock, ./ ]                      0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/10] RUN pipenv install --system --deploy                    0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/10] RUN pip install click                                   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/10] RUN mkdir -p /root/.aws                                 0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/10] RUN echo \"[default]\\naws_access_key_id = foo\\naws_secr  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 8/10] RUN echo \"[default]\\nregion = us-east-1\" > /root/.aws/  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 9/10] COPY [ batch.py, batch.py ]                             0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [10/10] COPY [ model.bin, model.bin ]                           0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:cee9a435043179367bc1b888d7e365faf24aa2cca4dda  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/hw6_image                               0.0s\n",
      "\u001b[0m\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! docker build -t hw6_image ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY              TAG       IMAGE ID       CREATED          SIZE\n",
      "hw6_image               latest    cee9a4350431   17 seconds ago   846MB\n",
      "localstack/localstack   latest    358a215f0f3b   2 days ago       1.09GB\n"
     ]
    }
   ],
   "source": [
    "! docker image ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/fsspec/registry.py:279: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n",
      "Data loaded from S3, INPUT_FILE_PATTERN is s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\n",
      "predicted mean duration: 14.292282936862449\n",
      "File saved to S3, OUTPUT_FILE_PATTERN is s3://nyc-duration/out/{year:04d}-{month:02d}.parquet\n"
     ]
    }
   ],
   "source": [
    "! docker run --network host -it \\\n",
    "                --env INPUT_FILE_PATTERN=\"s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\" \\\n",
    "                --env OUTPUT_FILE_PATTERN=\"s3://nyc-duration/out/{year:04d}-{month:02d}.parquet\" \\\n",
    "                --env S3_ENDPOINT_URL=\"http://localhost:4566\" \\\n",
    "                hw6_image --year 2023 --month 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. Creating test \n",
    "We'll use the dataframe we created in Q3 (the dataframe for the unit test)\n",
    "and save it to S3. You don't need to do anything else: just create a dataframe \n",
    "and save it.\n",
    "\n",
    "We will pretend that this is data for January 2023.\n",
    "What's the size of the file?\n",
    "* 3620 ✅\n",
    "* 23620\n",
    "* 43620\n",
    "* 63620"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "! touch integration_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for `integration_test.py`\n",
    "<details>\n",
    "<summary>Click to show/hide code</summary>\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from batch import get_input_path\n",
    "\n",
    "\n",
    "def dt(hour, minute, second=0):\n",
    "    return datetime(2023, 1, 1, hour, minute, second)\n",
    "\n",
    "\n",
    "data = [\n",
    "    (None, None, dt(1, 1), dt(1, 10)),\n",
    "    (1, 1, dt(1, 2), dt(1, 10)),\n",
    "    (1, None, dt(1, 2, 0), dt(1, 2, 59)),\n",
    "    (3, 4, dt(1, 2, 0), dt(2, 2, 1))\n",
    "]\n",
    "\n",
    "columns_test_df = ['PULocationID', 'DOLocationID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    "test_df = pd.DataFrame(data, columns=columns_test_df)\n",
    "\n",
    "s3_endpoint_url = os.getenv('S3_ENDPOINT_URL')\n",
    "options = {'client_kwargs': {'endpoint_url': s3_endpoint_url}}\n",
    "\n",
    "input_file = get_input_path(2023, 1)\n",
    "test_df.to_parquet(\n",
    "    input_file,\n",
    "    engine='pyarrow',\n",
    "    compression=None,\n",
    "    index=False,\n",
    "    storage_options=options\n",
    ")\n",
    "print(f'File saved to S3, input_file is {input_file}')\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "! touch run_integration_test_py_local.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for `run_integration_test_py_local.sh`\n",
    "<details>\n",
    "<summary>Click to show/hide code</summary>\n",
    "\n",
    "```shell\n",
    "#!/bin/bash\n",
    "\n",
    "# Set environment variables\n",
    "export INPUT_FILE_PATTERN=\"s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\"\n",
    "export S3_ENDPOINT_URL=\"http://localhost:4566\"\n",
    "\n",
    "# Run the test python script\n",
    "python integration_test.py\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x run_integration_test_py_local.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to S3, input_file is s3://nyc-duration/in/2023-01.parquet\n"
     ]
    }
   ],
   "source": [
    "! ./run_integration_test_py_local.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3620\n"
     ]
    }
   ],
   "source": [
    "! aws --endpoint-url=http://localhost:4566 s3api head-object --bucket nyc-duration --key in/2023-01.parquet --query 'ContentLength'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Finish the integration test\n",
    "Let's run the `batch.py` script for January 2023 (the fake data we created in Q5)\n",
    "\n",
    "What's the sum of predicted durations for the test dataframe?\n",
    "* 13.08\n",
    "* 36.28 ✅\n",
    "* 69.28\n",
    "* 81.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from S3, INPUT_FILE_PATTERN is s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\n",
      "predicted mean duration: 18.138625226015364\n",
      "File saved to S3, OUTPUT_FILE_PATTERN is s3://nyc-duration/out/{year:04d}-{month:02d}.parquet\n"
     ]
    }
   ],
   "source": [
    "# Change month variable in run_batch_py_local.sh\n",
    "! ./run_batch_py_local.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>predicted_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023/01_0</td>\n",
       "      <td>23.197149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023/01_1</td>\n",
       "      <td>13.080101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ride_id  predicted_duration\n",
       "0  2023/01_0           23.197149\n",
       "1  2023/01_1           13.080101"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_endpoint_url = 'http://localhost:4566'\n",
    "options = {'client_kwargs': {'endpoint_url': s3_endpoint_url}}\n",
    "\n",
    "year, month = 2023, 1\n",
    "output_file = f\"s3://nyc-duration/out/{year:04d}-{month:02d}.parquet\"\n",
    "df = pd.read_parquet(output_file, storage_options=options)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(36.28)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.predicted_duration.sum().round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
